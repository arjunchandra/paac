import numpy as np
import random

import logging
logger = logging.getLogger()

from segment_tree import SumSegmentTree, MinSegmentTree


class ReplayBuffer(object):
    def __init__(self, size, dsize, n_emus=1, debug = False):
        """Create Prioritized Replay buffer.

        Parameters
        ----------
        size: int
            Max number of transitions to store in the buffer. The space for self-generated transitions 
            will approximately be size - dsize 
            When the buffer overflows the old self-generated memories are dropped (the memories
            from demonstrations are always retained)
        dsize: int
            Number of demonstration transitions. These are retained in the 
            buffer permanently.
            Correct behavior of the buffer assumes that dsize demo transitions will actually be stored in the buffer
            when beginning to sample.
            https://arxiv.org/abs/1704.03732
        n_emus: int
            Number of emulators. Each emulator will store samples in its own buffer, 
            but sampling will happen accross buffers 
        """

        # The buffer is logically divided in n_emu + 1 segments, where the
        # first segment is reserved for demo data and each of the other segments (of equal size) are reserved for data
        # generated by each of the emulators

        # We need to ensure that each emulator segment has the same size
        self._emu_buffer_size = round( (size - dsize) / n_emus)
        self._selfgen_size = self._emu_buffer_size * n_emus
        self._max_size = self._selfgen_size + dsize
        self._demo_size = dsize
        #self._demo_size_minusone = self._demo_size - 1

        self._n_emu = n_emus

        # The buffer where the actual data is stored.
        self._storage = [0]*self._max_size

        # Number of trasitions in each of emulator sub-buffer. Needed for sampling.
        # When beginning to sample, it is assume that each emulator sub-buffer has the same number of samples
        self._n_samples = 0

        self._next_idx_emu = [0]*n_emus
        self._next_idx_demo = 0
        # This will keep track of the actual index in self._storage where data is added. Needed for PrioritizedReplayBuffer
        #self._next_idx = 0
        #self._traj_id = 0

        self.debug = debug


    def __len__(self):
        return self._n_samples

    # emu_id: 0-based id of the emulator adding the sample
    def add(self, obs_t, action, reward, obs_tp1, done, emu_id):
        data = (obs_t, action, reward, obs_tp1, done) #, self._traj_id)

        idx = self._demo_size + emu_id*self._emu_buffer_size + self._next_idx_emu[emu_id]
        self._storage[idx] = data

        # cyclic storage inside each emulator's sub-buffer
        self._next_idx_emu[emu_id] = (self._next_idx_emu[emu_id] + 1) % (self._emu_buffer_size)

        if emu_id == 0 and self._n_samples < self._emu_buffer_size:
            self._n_samples += 1

        if self.debug:
            print("ADDING TO BUFFER emu {}: State".format(emu_id), np.argmax(self._storage[idx][0].flatten()))
            s = []
            for i in range(len(self._storage)):
                if type(self._storage[i]) is not int:
                    s.append(np.argmax(self._storage[i][0].flatten()))
                else:
                    s.append(-1)
            print("STATES IN BUFFER: ", s)
        return idx


    def add_demo(self, obs_t, action, reward, obs_tp1, done):
        data = (obs_t, action, reward, obs_tp1, done)
        self._storage[self._next_idx_demo] = data

        idx = self._next_idx_demo
        self._next_idx_demo = (self._next_idx_demo + 1) % self._demo_size
        if self.debug:
            s = []
            for i in range(len(self._storage)):
                if type(self._storage[i]) is not int:
                    s.append(np.argmax(self._storage[i][0].flatten()))
                else:
                    s.append(-1)
            print("BUFFER AFTER ADDING DEMO: ", s)
        return idx


    def _retrieve_samples(self, idxes):
        obses_t, actions, rewards, obses_tp1, dones = [], [], [], [], []
        for i in idxes:
            data = self._storage[i]
            obs_t, action, reward, obs_tp1, done, traj_id = data
            obses_t.append(obs_t)
            actions.append(action)
            rewards.append(reward)
            obses_tp1.append(obs_tp1)
            dones.append(done)
        return np.array(obses_t), np.array(actions), np.array(rewards), np.array(obses_tp1), np.array(dones)

    def _retrieve_n_step_trajectories(self, traj_idxes, n_steps):
        idxes, batch_obses_t, batch_actions, batch_rewards, batch_obses_tp1, batch_dones = [], [], [], [], [], []

        # For each provided index, we will retrieve n-steps samples starting from that index, or containing
        # that index
        max_allowed_emu_idx = self._n_samples - n_steps
        max_allowed_demo_idx = self._demo_size - n_steps
        for idx in traj_idxes:
            try:
                if idx < self._demo_size:
                    idx = min(idx, max_allowed_demo_idx)
                    #emu_id = 0
                else:
                    #emu_id = (idx - self._demo_size) // self._emu_buffer_size
                    _idx = (idx - self._demo_size) % self._emu_buffer_size
                    idx -= max(0, _idx - max_allowed_emu_idx)

                n_step_obses_t, n_step_actions, n_step_rewards, n_step_obses_tp1, n_step_dones = [], [], [], [], []
                for j in range(idx, idx+n_steps):
                    idxes.append(j)
                    data = self._storage[j]
                    obs_t, action, reward, obs_tp1, done = data
                    n_step_obses_t.append(obs_t)
                    n_step_actions.append(action)
                    n_step_rewards.append(reward)
                    n_step_obses_tp1.append(obs_tp1)
                    n_step_dones.append(done)
                    
                batch_obses_t.append(n_step_obses_t)
                batch_actions.append(n_step_actions)
                batch_rewards.append(n_step_rewards)
                batch_obses_tp1.append(n_step_obses_tp1)
                batch_dones.append(n_step_dones)
            except IndexError:
                print("Something funky happened accessing replay storage.")
        ##print("Inside buffer: ", np.array(batch_obses_t).shape)
        return [np.array(batch_obses_t), np.array(batch_actions), np.array(batch_rewards), np.array(batch_obses_tp1), np.array(batch_dones)], np.array(idxes)

    def _get_indexes(self, n):
        if self._demo_size == 0:
            # Sample the sub-buffers.
            buff_id = np.array([random.randint(0, self._n_emu-1) for _ in range(n)])
            demo_mask = np.zeros(n)
            # Sample from the emulator sub-buffers
            # Obs! self._n_samples is the number of transitions on any emulator sub-buffer
            emu_idxes = np.array([random.randint(0, self._n_samples - 1) for _ in range(n)])
            idxes = buff_id * self._emu_buffer_size + emu_idxes
            #print("Buff id", buff_id)
            #print("emu_idxes", emu_idxes)
            #print("idxes", idxes)

        elif self._n_samples == 0:
            # There is only demo data in the buffer, so we sample from those
            idxes = np.array([random.randint(0, self._demo_size -1) for _ in range(n)])
            demo_mask = np.ones(n)
        else:
            # Sample the sub-buffers.
            buff_id = np.array([random.randint(0, self._n_emu) for _ in range(n)])
            # We consider the demo sub-buffer has id = self._n_emu, just for the sake of sampling
            demo_mask = (buff_id == self._n_emu)
            emu_mask = 1 - demo_mask

            # Sample from the demo sub-buffer
            demo_idxes = np.array([random.randint(0, self._demo_size -1) for _ in range(n)])
            # Sample from the emulator sub-buffers
            # Obs! self._n_samples = number of transitions on any emulator sub-buffer
            emu_idxes = np.array([random.randint(0, self._n_samples - 1) for _ in range(n)])
            emu_idxes = self._demo_size + buff_id * self._emu_buffer_size + emu_idxes

            idxes = (demo_idxes*demo_mask) + (emu_idxes*emu_mask)

        return idxes, demo_mask

    def sample(self, batch_size):
        """Sample a batch of experiences.
        For correct behavior, the same number of transitions for each emulator should be stored in the buffer 
        when sampling 

        Parameters
        ----------
        batch_size: int
            How many transitions to sample.

        Returns
        -------
        obs_batch: np.array
            batch of observations
        act_batch: np.array
            batch of actions executed given obs_batch
        rew_batch: np.array
            rewards received as results of executing act_batch
        next_obs_batch: np.array
            next set of observations seen after executing act_batch
        done_mask: np.array
            done_mask[i] = 1 if executing act_batch[i] resulted in
            the end of an episode and 0 otherwise.
        """
        idxes, demo_mask = self._get_indexes(batch_size)
        return self._retrieve_samples(idxes)

    def sample_nstep(self, n_trajectories, n_steps):
        """Sample n_trajectories * n_steps experiences

        Parameters
        ----------
        n_trajectories: int
            How many trajectories to sample.
        n_steps: int
            How many steps to retrieve for each trajectory

        Returns
        -------
        obs_batch: np.array
            batch of observations
        act_batch: np.array
            batch of actions executed given obs_batch
        rew_batch: np.array
            rewards received as results of executing act_batch
        next_obs_batch: np.array
            next set of observations seen after executing act_batch
        done_mask: np.array
            done_mask[i] = 1 if executing act_batch[i] resulted in
            the end of an episode and 0 otherwise.
        n_step_rewards_batch: np.array
            n-step rewards vector batch
        tpn_obs_batch: np.array
            tpn set of observations
        n_tpn_step_batch: np.array
            n in n-step indicator to indicate if trajectory sampled 
            is unfinished or done -- trajectory is unfinished if 
            there are no more transitions to cover all n steps
        n_step_done_mask: np.array
            n_step_done_mask[i] = 1 if trajectory sampled reaches 
            the end of an episode, and 0 otherwise.
        """
        s=[]
        for i in range(len(self._storage)):
            if type(self._storage[i]) is not int:
                s.append(np.argmax(self._storage[i][0].flatten()))
        print("STATES IN BUFFER: ", s)


        traj_idxes, demo_mask = self._get_indexes(n_trajectories)
        trajectories, idxes = self._retrieve_n_step_trajectories(traj_idxes, n_steps)
        return tuple(trajectories + [idxes, demo_mask])

class PrioritizedReplayBuffer(ReplayBuffer):
    def __init__(self, size, dsize, alpha, n_emus=1, debug = False):
        """Create Prioritized Replay buffer.

        Parameters
        ----------
        size: int
            Max number of transitions to store in the buffer. When the buffer
            overflows the old memories are dropped.
        dsize: int
            Max number of demonstration transitions. These are retained in the 
            buffer permanently.
            https://arxiv.org/abs/1704.03732
        alpha: float
            how much prioritization is used
            (0 - no prioritization, 1 - full prioritization)

        See Also
        --------
        ReplayBuffer.__init__
        """
        super(PrioritizedReplayBuffer, self).__init__(size, dsize, n_emus=n_emus)
        assert alpha > 0
        self._alpha = alpha

        it_capacity = 1
        while it_capacity < size:
            it_capacity *= 2

        self._it_sum = SumSegmentTree(it_capacity)
        self._it_min = MinSegmentTree(it_capacity)
        self._max_priority = 1.0

        self.debug = debug

    def add(self, *args, **kwargs):
        """See ReplayBuffer.add_effect"""
        idx = super().add(*args, **kwargs)
        self._it_sum[idx] = self._max_priority ** self._alpha
        self._it_min[idx] = self._max_priority ** self._alpha

    def add_demo(self, *args, **kwargs):
        """See ReplayBuffer.add_demo_effect"""
        idx = super().add_demo(*args, **kwargs)
        self._it_sum[idx] = self._max_priority ** self._alpha
        self._it_min[idx] = self._max_priority ** self._alpha

    def _sample_proportional(self, batch_size):
        res = []
        for _ in range(batch_size):
            # TODO(szymon): should we ensure no repeats?
            mass = random.random() * self._it_sum.sum(0, len(self._storage) - 1)
            idx = self._it_sum.find_prefixsum_idx(mass)
            res.append(idx)
        return np.array(res)

    def _compute_weights(self, idxes, beta):
        weights = []
        p_min = self._it_min.min() / self._it_sum.sum()
        max_weight = (p_min * len(self._storage)) ** (-beta)

        for idx in idxes:
            p_sample = self._it_sum[idx] / self._it_sum.sum()
            weight = (p_sample * len(self._storage)) ** (-beta)
            weights.append(weight / max_weight)

        return np.array(weights)

    def sample(self, batch_size, beta):
        """Sample a batch of experiences.

        compared to ReplayBuffer.sample
        it also returns importance weights and idxes
        of sampled experiences.


        Parameters
        ----------
        batch_size: int
            How many transitions to sample.
        beta: float
            To what degree to use importance weights
            (0 - no corrections, 1 - full correction)

        Returns
        -------
        obs_batch: np.array
            batch of observations
        act_batch: np.array
            batch of actions executed given obs_batch
        rew_batch: np.array
            rewards received as results of executing act_batch
        next_obs_batch: np.array
            next set of observations seen after executing act_batch
        done_mask: np.array
            done_mask[i] = 1 if executing act_batch[i] resulted in
            the end of an episode and 0 otherwise.
        weights: np.array
            Array of shape (batch_size,) and dtype np.float32
            denoting importance weight of each sampled transition
        idxes: np.array
            Array of shape (batch_size,) and dtype np.int32
            idexes in buffer of sampled experiences
        """
        assert beta > 0

        idxes = self._sample_proportional(batch_size)
        batch_samples = self._retrieve_samples(idxes)
        weights = self._compute_weights(idxes, beta)
        return tuple(list(batch_samples) + [weights, idxes])

    def sample_nstep(self, n_trajectories, n_steps, beta):
        """Sample Sample n_trajectories * n_steps experiences.

        Compared to ReplayBuffer.sample
        it also returns importance weights and idxes
        of sampled experiences.


        Parameters
        ----------
        batch_size: int
            How many transitions to sample.
        beta: float
            To what degree to use importance weights
            (0 - no corrections, 1 - full correction)
        n_step: int
            How many steps to look into the future

        Returns
        -------
        obs_batch: np.array
            batch of observations
        act_batch: np.array
            batch of actions executed given obs_batch
        rew_batch: np.array
            rewards received as results of executing act_batch
        next_obs_batch: np.array
            next set of observations seen after executing act_batch
        done_mask: np.array
            done_mask[i] = 1 if executing act_batch[i] resulted in
            the end of an episode and 0 otherwise.
        n_step_rewards_batch: np.array
            n-step rewards vector batch
        tpn_obs_batch: np.array
            tpn set of observations
        n_tpn_step_batch: np.array
            n in n-step indicator to indicate if trajectory sampled 
            is unfinished or done -- trajectory is unfinished if 
            there are no more transitions to cover all n steps
        n_step_done_mask: np.array
            n_step_done_mask[i] = 1 if trajectory sampled reaches 
            the end of an episode, and 0 otherwise.
        weights: np.array
            Array of shape (batch_size,) and dtype np.float32
            denoting importance weight of each sampled transition
        idxes: np.array
            Array of shape (batch_size,) and dtype np.int32
            idexes in buffer of sampled experiences
        """
        assert beta > 0

        traj_idxes = self._sample_proportional(n_trajectories)
        batched_trajectories, idxes = self._retrieve_n_step_trajectories(traj_idxes, n_steps)
        if self.debug: print("TRAJ. IDXES: ", traj_idxes)
        weights = self._compute_weights(idxes, beta)
        demo_mask = (idxes < self._demo_size)
        return tuple(batched_trajectories + [weights, idxes, demo_mask])

    def update_priorities(self, idxes, priorities):
        """Update priorities of sampled transitions.

        sets priority of transition at index idxes[i] in buffer
        to priorities[i].

        Parameters
        ----------
        idxes: [int]
            List of idxes of sampled transitions
        priorities: [float]
            List of updated priorities corresponding to
            transitions at the sampled idxes denoted by
            variable `idxes`.
        """
        assert len(idxes) == len(priorities)
        for idx, priority in zip(idxes, priorities):
            assert priority > 0
            assert 0 <= idx < len(self._storage)
            self._it_sum[idx] = priority ** self._alpha
            self._it_min[idx] = priority ** self._alpha

            self._max_priority = max(self._max_priority, priority)


if __name__ == '__main__':
    replay_buffer = PrioritizedReplayBuffer(10, 0, alpha=0.6)
    for i in range(5):
        replay_buffer.add(i, i, i, i, i)
    experience = replay_buffer.sample(2, beta=0.1)
    #print(experience)


